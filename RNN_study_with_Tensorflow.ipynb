{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN study with Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cmajorsolo/LSTM-crypto-currency/blob/master/RNN_study_with_Tensorflow.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "kPVrmF-OA0VJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "127de715-f77f-49a7-d2af-be43ea5988a2"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "x1 = tf.constant(5)\n",
        "x2 = tf.constant(6)\n",
        "# result = x1 * x2\n",
        "result = tf.multiply(x1, x2)\n",
        "print(result)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  output = sess.run(result)\n",
        "print(output)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Mul_9:0\", shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a-DjB5yPRxkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "aee257d0-7985-41f4-e9d1-ae89cd718cc2"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "build a neural network model\n",
        "input > weight >hidden layer 1 (activation function) > weights > hidden layer 2(activation function) > weights > output layer\n",
        "\n",
        "compare output to intended output > cost of loss function(corss entropy)\n",
        "optimization function (optimizer) > minimize cost (AdamOptimizer....SGD, AdaGrad)\n",
        "\n",
        "backpropagation\n",
        "\n",
        "feed forward + backprop = epoch\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets('/tmp/data', one_hot = True)\n",
        "\n",
        "n_nodes_hl1 = 500  #500 nodes on hidden layer 1\n",
        "n_nodes_hl2 = 500  #500 nodes on hidden layer 2\n",
        "n_nodes_hl3 = 500  #500 nodes on hidden layer 3\n",
        "\n",
        "n_classes = 10  #10 numbers\n",
        "batch_size = 100  #batch is the feature size. feed in one batch, update weights then repeat. \n",
        "\n",
        "x = tf.placeholder('float', [None, 784])\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "def neural_network_model(data):\n",
        "  # (input_data * weights) + biases\n",
        "  # if input data is 0, input_data * weights will be 0. No neurals will be fired\n",
        "  # adding biases here to fire the neurons in this case\n",
        "  hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])), \n",
        "                    'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "  hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
        "                    'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "  hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
        "                    'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
        "  output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n",
        "                    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
        "  # model for each layer (input_data * weights) + biases\n",
        "  l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])  #matrix multiplication\n",
        "  l1 = tf.nn.relu(l1)  #activation function\n",
        "  \n",
        "  l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])  #matrix multiplication\n",
        "  l2 = tf.nn.relu(l2)  #activation function\n",
        "  \n",
        "  l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])  #matrix multiplication\n",
        "  l2 = tf.nn.relu(l3)  #activation function\n",
        "\n",
        "  output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
        "  \n",
        "  return output\n",
        "\n",
        "def train_neural_network(x):\n",
        "  prediction = neural_network_model(x)\n",
        "  cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
        "  \n",
        "  # learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "  \n",
        "  # cycles feed forward + backprop\n",
        "  hm_epochs = 10\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    \n",
        "    for epoch in range(hm_epochs):\n",
        "      epoch_loss = 0\n",
        "      for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "        epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "        _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y}) # c is the cost\n",
        "        epoch_loss += c\n",
        "      print('Epoch', epoch, 'completed out of ', hm_epochs, 'loss:', epoch_loss)\n",
        "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))  #argmx returns max value\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "    print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
        "\n",
        "train_neural_network(x)    \n",
        "      "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch 0 completed out of  10 loss: 2739163.952392578\n",
            "Epoch 1 completed out of  10 loss: 654612.3142814636\n",
            "Epoch 2 completed out of  10 loss: 380246.40410614014\n",
            "Epoch 3 completed out of  10 loss: 250210.49446150474\n",
            "Epoch 4 completed out of  10 loss: 164160.6193971634\n",
            "Epoch 5 completed out of  10 loss: 116557.556773711\n",
            "Epoch 6 completed out of  10 loss: 93931.12151110172\n",
            "Epoch 7 completed out of  10 loss: 70697.65365123749\n",
            "Epoch 8 completed out of  10 loss: 53436.427868396044\n",
            "Epoch 9 completed out of  10 loss: 45325.36055856943\n",
            "Accuracy: 0.9574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TwDCKjVeSs0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "35099a2a-9136-46e7-c069-2fd6ccdcf328"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oaaEVrLncYVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "13f94583-fbd6-481d-d68e-b1f9df142ae4"
      },
      "cell_type": "code",
      "source": [
        "# local data file is located at /content/gdrive/My\\ Drive/Colab\\ Notebooks\\neg.txt\n",
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> tokenize\n",
            "    Error loading tokenize: Package 'tokenize' not found in index\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> stem\n",
            "Command 'stem' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> all\n",
            "Command 'all' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VxwmdHq8hjjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "c4eaa1ac-def9-4d75-b570-d220f7b20b8d"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import work_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "hm_lines = 10000000\n",
        "\n",
        "def create_lexicon(pos, neg):\n",
        "  lexicon = []\n",
        "  for fi in [pos, neg]:\n",
        "    with open(fi, 'r') as f:\n",
        "      contents = f.readlines()\n",
        "      for l in contents[:hm_lines]:\n",
        "        all_words = word_tokenize(l.lower())\n",
        "        lexicon += list(all_words)\n",
        "  lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
        "  w_counts = Counter(lexicon)\n",
        "  l2 = []\n",
        "  for w in w_counts:\n",
        "    if 1000 > w_counts[w] > 50:\n",
        "      l2.append(w)\n",
        "      \n",
        "  return l2\n",
        "\n",
        "\n",
        "def sample_handling(sample, lexicon, classification):\n",
        "  featureset = []\n",
        "  with open(sample, 'r') as f:\n",
        "    contents = f.readlines()\n",
        "    for l in contents[:hm_lines]:\n",
        "      current_words = word_tokenize(l.lower())\n",
        "      current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "      features = np.zeros(len(lexicon))\n",
        "      for word in current_words:\n",
        "        if word.lower() in lexicon:\n",
        "          index_value = lexicon.index(word.lower())\n",
        "          features[index_value] += 1\n",
        "      features = list(features)\n",
        "      featureset.append([features, classification])\n",
        "      \n",
        "  return featureset\n",
        "\n",
        "def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n",
        "  lexicon = create_lexicon(pos, neg)\n",
        "  features = []\n",
        "  features += sample_handling('/content/gdrive/My\\ Drive/Colab\\ Notebooks\\pos.txt', lexicon, [1,0])\n",
        "  features += sample_handling('/content/gdrive/My\\ Drive/Colab\\ Notebooks\\neg.txt', lexicon, [0,1])\n",
        "  random.shuffle(features)\n",
        "  \n",
        "  features = np.array(features)\n",
        "  testing_size = int(test_size*len(features))\n",
        "  train_x = list(features[:,0][:-testing_size])\n",
        "  train_y = list(features[:,1][:-testing_size])\n",
        "  test_x = list(features[:,0][:-testing_size])\n",
        "  test_y = list(features[:,1][:-testing_size])\n",
        "  \n",
        "  return train_x, train_y, test_x, test_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_x, train_y, test_x, test_y = create_feature_sets_and_labels('/content/gdrive/My\\ Drive/Colab\\ Notebooks\\pos.txt', '/content/gdrive/My\\ Drive/Colab\\ Notebooks\\neg.txt')\n",
        "  with open('/content/gdrive/My\\ Drive/Colab\\sentiment_set.pickle', 'wb') as f:\n",
        "    pickle.dump([train_x, train_y, test_x, test_y], f)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e8835814bb6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwork_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'work_tokenize'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xrqWW_BwUV28",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Using RNN to predict cryptocurrency price**"
      ]
    },
    {
      "metadata": {
        "id": "gbvw0uze29fV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9a5e7d0f-f681-4689-fea8-0dc2b145072f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1iKXPpsBt-Dm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8117e00b-d49f-43de-fc7f-9339ab1f899a"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardcolab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardcolab\n",
            "  Downloading https://files.pythonhosted.org/packages/73/3d/eaf745e162e471c5bb2737a407d8626fb8684a88cf085045456aeb841d3c/tensorboardcolab-0.0.19.tar.gz\n",
            "Building wheels for collected packages: tensorboardcolab\n",
            "  Running setup.py bdist_wheel for tensorboardcolab ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ab/74/02/cda602d1dc28b2f12eab313c49b9bfa14d6371326bc2590e06\n",
            "Successfully built tensorboardcolab\n",
            "Installing collected packages: tensorboardcolab\n",
            "Successfully installed tensorboardcolab-0.0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QaWsAORc7LCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1d680361-a153-4c8d-fe0a-c382ef3fe514"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "print(tf.test.is_gpu_available())\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "\n",
        "SEQ_LEN = 60\n",
        "FUTURE_PERIOD_PREDICT = 3\n",
        "RATIO_TO_PREDICT = 'LTC-USD'\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "  if float(future) > float(current):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def preprocess_df(df):\n",
        "  df = df.drop('future', 1)\n",
        "#   scalling columns\n",
        "  for col in df.columns:\n",
        "    if col != 'target':\n",
        "      df[col] = df[col].pct_change()\n",
        "      df.dropna(inplace=True)\n",
        "      df[col] = preprocessing.scale(df[col].values)\n",
        "  df.dropna(inplace=True)\n",
        "  \n",
        "  sequential_data = []\n",
        "  prev_days = deque(maxlen=SEQ_LEN)\n",
        "#   print(df.head())\n",
        "#   for c in df.columns:\n",
        "#     print(c)\n",
        "  for i in df.values:\n",
        "    prev_days.append([n for n in i[:-1]])\n",
        "    if len(prev_days) == SEQ_LEN:\n",
        "      sequential_data.append([np.array(prev_days), i[-1]])\n",
        "  random.shuffle(sequential_data)\n",
        "#   balancing the data\n",
        "  buys = []\n",
        "  sells = []\n",
        "  for seq, target in sequential_data:\n",
        "    if target == 0:\n",
        "      sells.append([seq, target])\n",
        "    elif target == 1:\n",
        "      buys.append([seq, target])\n",
        "  random.shuffle(buys)\n",
        "  random.shuffle(sells)\n",
        "  \n",
        "  lower = min(len(buys), len(sells))\n",
        "  buys = buys[:lower]\n",
        "  sells = sells[:lower]\n",
        "  sequential_data = buys + sells\n",
        "  random.shuffle(sequential_data)\n",
        "  \n",
        "  X = []\n",
        "  y = []\n",
        "  \n",
        "  for seq, target in sequential_data:\n",
        "    X.append(seq)\n",
        "    y.append(target)\n",
        "    \n",
        "  return np.array(X), y\n",
        "\n",
        "\n",
        "# df = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/crypto_data/LTC-USD.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
        "# print(df.head())\n",
        "main_df = pd.DataFrame()\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]\n",
        "for ratio in ratios:\n",
        "#   print(ratio)\n",
        "  dataset = f'/content/gdrive/My Drive/Colab Notebooks/crypto_data/{ratio}.csv'\n",
        "  df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
        "  df.rename(columns={\"close\":f\"{ratio}_close\", \"volume\":f\"{ratio}_volume\"}, inplace=True)\n",
        "  df.set_index('time',inplace=True)\n",
        "  df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
        "#   print(df.head())\n",
        "  \n",
        "  if len(main_df) == 0:\n",
        "    main_df = df\n",
        "  else:\n",
        "    main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method='ffill', inplace=True)\n",
        "main_df.dropna(inplace=True)\n",
        "# print(main_df.head())\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "# print(main_df.head())\n",
        "\n",
        "times = sorted(main_df.index.values)\n",
        "last_5pct = times[-int(0.05*len(times))]\n",
        "# print(last_5pct)\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "# print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "# print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "# print(f\"VALIDATION DONt buys: {validation_y.count(0)}, buys:{validation_y.count(1)}\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "tensorboard = TensorBoard(log_dir=f'/content/gdrive/My Drive/Colab Notebooks/crypto_data/{NAME}')\n",
        "# tbc=TensorBoardColab()\n",
        "\n",
        "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"\n",
        "checkpoint = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/crypto_data/{}.model'.format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y),\n",
        "#     callbacks=[TensorBoardColabCallback(tbc), checkpoint],\n",
        "    callbacks=[tensorboard, checkpoint],\n",
        ")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Train on 77922 samples, validate on 3860 samples\n",
            "Epoch 1/10\n",
            "77922/77922 [==============================] - 84s 1ms/step - loss: 0.7079 - acc: 0.5251 - val_loss: 0.6886 - val_acc: 0.5461\n",
            "Epoch 2/10\n",
            "77922/77922 [==============================] - 70s 904us/step - loss: 0.6899 - acc: 0.5354 - val_loss: 0.6909 - val_acc: 0.5034\n",
            "Epoch 3/10\n",
            "77922/77922 [==============================] - 71s 908us/step - loss: 0.6846 - acc: 0.5543 - val_loss: 0.6889 - val_acc: 0.5497\n",
            "Epoch 4/10\n",
            "77922/77922 [==============================] - 68s 878us/step - loss: 0.6819 - acc: 0.5629 - val_loss: 0.6784 - val_acc: 0.5617\n",
            "Epoch 5/10\n",
            "77922/77922 [==============================] - 71s 906us/step - loss: 0.6802 - acc: 0.5664 - val_loss: 0.6777 - val_acc: 0.5580\n",
            "Epoch 6/10\n",
            "77922/77922 [==============================] - 71s 909us/step - loss: 0.6794 - acc: 0.5674 - val_loss: 0.6776 - val_acc: 0.5731\n",
            "Epoch 7/10\n",
            "77922/77922 [==============================] - 69s 890us/step - loss: 0.6782 - acc: 0.5716 - val_loss: 0.6726 - val_acc: 0.5710\n",
            "Epoch 8/10\n",
            "77922/77922 [==============================] - 68s 879us/step - loss: 0.6756 - acc: 0.5763 - val_loss: 0.6720 - val_acc: 0.5876\n",
            "Epoch 9/10\n",
            "77922/77922 [==============================] - 69s 884us/step - loss: 0.6726 - acc: 0.5806 - val_loss: 0.6751 - val_acc: 0.5769\n",
            "Epoch 10/10\n",
            "77922/77922 [==============================] - 68s 869us/step - loss: 0.6683 - acc: 0.5901 - val_loss: 0.6718 - val_acc: 0.5842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7g9ILRp3mJ16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "886a2dc3-39ac-4fa9-bc0d-47e077dadf07"
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "from google.colab import files\n",
        "\n",
        "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/gdrive/My\\ Drive/foo.txt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/crypto_data/LTC-USD.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
        "print(df.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!         time        low       high       open      close      volume\n",
            "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
            "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
            "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
            "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
            "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}